# IntroduÃ§Ã£o Ã  AnÃ¡lise de Dados em SaÃºde com PySpark

Este repositÃ³rio contÃ©m materiais prÃ¡ticos para **anÃ¡lise de dados em larga escala utilizando PySpark**, com foco em **dados do SUS (DATASUS / SIM)**. Ele Ã© voltado a estudantes, pesquisadores e profissionais de saÃºde pÃºblica que desejam trabalhar com **big data**, pipelines distribuÃ­dos e anÃ¡lises reprodutÃ­veis em ambientes locais ou em nuvem.

O conteÃºdo Ã© uma **adaptaÃ§Ã£o do curso â€œIntroduÃ§Ã£o Ã  AnÃ¡lise de Dados para Pesquisa no SUSâ€**, originalmente baseado em R, agora reestruturado para **Apache Spark + Python (PySpark)**.

---

## ğŸ“š ESTRUTURA DO REPOSITÃ“RIO

O repositÃ³rio estÃ¡ organizado em **mÃ³dulos progressivos**, do bÃ¡sico ao avanÃ§ado:

| MÃ³dulo | Tema | Status |
|------|------|--------|
| **MÃ³dulo 1** | Fundamentos de PySpark e DataFrames | âœ… |
| **MÃ³dulo 2** | AnÃ¡lise ExploratÃ³ria e EstatÃ­stica Descritiva | âœ… |
| **MÃ³dulo 3** | Engenharia de Dados e Modelagem | â³ |

---

## ğŸ“ ESTRUTURA DE PASTAS

```text
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # Dados brutos (DATASUS, SIM, etc.)
â”‚   â”œâ”€â”€ processed/        # Dados tratados
â”‚   â””â”€â”€ dictionary/       # DicionÃ¡rios de dados
â”‚
â”œâ”€â”€ notebooks/            # Notebooks Jupyter (exploraÃ§Ã£o e estudo)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/        # Leitura e padronizaÃ§Ã£o de dados
â”‚   â”œâ”€â”€ transformation/   # Limpeza e transformaÃ§Ã£o
â”‚   â”œâ”€â”€ analysis/         # AnÃ¡lises estatÃ­sticas
â”‚   â””â”€â”€ utils/            # FunÃ§Ãµes utilitÃ¡rias
â”‚
â”œâ”€â”€ tests/                # Testes automatizados
â”œâ”€â”€ requirements.txt      # DependÃªncias do projeto
â””â”€â”€ README.md
```

---

## ğŸ“Š DADOS UTILIZADOS

Os exemplos utilizam dados do **Sistema de InformaÃ§Ãµes sobre Mortalidade (SIM)**, em diferentes formatos.

| Arquivo | Formato | DescriÃ§Ã£o |
|------|------|-----------|
| `sim_salvador_2023.csv` | CSV | Dados originais do SIM |
| `sim_salvador_2023.parquet` | Parquet | Dataset otimizado para Spark |
| `sim_salvador_2023_processado.parquet` | Parquet | Dados tratados |
| `dicionario_sim.pdf` | PDF | DicionÃ¡rio de variÃ¡veis |

### Principais variÃ¡veis

- **SEXO** â€“ sexo do indivÃ­duo
- **DTOBITO** â€“ data do Ã³bito (`ddMMyyyy`)
- **DTNASC** â€“ data de nascimento
- **IDADE** â€“ idade codificada (padrÃ£o DATASUS)
- **CAUSABAS** â€“ causa bÃ¡sica (CID-10)
- **CODMUNRES** â€“ municÃ­pio de residÃªncia (IBGE)

---

## ğŸ¯ OBJETIVOS DE APRENDIZAGEM

### MÃ³dulo 1 â€“ Fundamentos de PySpark

- CriaÃ§Ã£o de `SparkSession`
- Leitura de dados CSV, Parquet e JSON
- Uso de `DataFrame`, `select`, `withColumn`, `filter`
- Tipagem de dados e tratamento de datas
- Boas prÃ¡ticas em Spark

### MÃ³dulo 2 â€“ EstatÃ­stica Descritiva DistribuÃ­da

- AgregaÃ§Ãµes com `groupBy` e `agg`
- EstatÃ­sticas descritivas em grandes volumes
- AnÃ¡lise temporal (ano, mÃªs, sÃ©ries histÃ³ricas)
- IntegraÃ§Ã£o com Pandas para visualizaÃ§Ã£o
- ExportaÃ§Ã£o de resultados

### MÃ³dulo 3 â€“ Engenharia de Dados e Modelagem

- Pipelines ETL com PySpark
- Particionamento e otimizaÃ§Ã£o
- ValidaÃ§Ã£o e qualidade dos dados
- IntegraÃ§Ã£o com dashboards (React / APIs)
- PreparaÃ§Ã£o de dados para modelos preditivos

---

## ğŸš€ COMO EXECUTAR O PROJETO

### 1. PrÃ©-requisitos

- Python 3.10+
- Java 8 ou 11
- Apache Spark 3.x

### 2. Criar ambiente virtual

```bash
python -m venv .venv
source .venv/bin/activate
```

### 3. Instalar dependÃªncias

```bash
pip install -r requirements.txt
```

### 4. Exemplo bÃ¡sico de execuÃ§Ã£o

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("SIM Analysis") \
    .getOrCreate()

df = spark.read.parquet("data/processed/sim_salvador_2023.parquet")
df.show(5)
```

---

## ğŸ§ª TRATAMENTO DE DATAS (EXEMPLO REAL)

```python
from pyspark.sql.functions import col, lpad, to_date, year

df = df.withColumn(
    "DTOBITO_clean",
    lpad(col("DTOBITO").cast("string"), 8, "0")
).withColumn(
    "DTOBITO_dt",
    to_date(col("DTOBITO_clean"), "ddMMyyyy")
).withColumn(
    "ano_obito",
    year(col("DTOBITO_dt"))
)
```

Esse padrÃ£o Ã© amplamente utilizado em bases do **DATASUS**, onde datas frequentemente perdem zeros Ã  esquerda.

---

## ğŸ”§ BOAS PRÃTICAS EM PYSPARK

- Prefira **Parquet** a CSV
- Evite `collect()` em grandes volumes
- Use `select` em vez de carregar todas as colunas
- Trate schemas explicitamente quando possÃ­vel
- Separe ingestÃ£o, transformaÃ§Ã£o e anÃ¡lise

---

## ğŸ”— REFERÃŠNCIAS E MATERIAIS DE APOIO

### DocumentaÃ§Ã£o

- Apache Spark: https://spark.apache.org/docs/latest/
- PySpark API: https://spark.apache.org/docs/latest/api/python/

### Dados de SaÃºde

- DATASUS â€“ https://datasus.saude.gov.br/
- OpenDataSUS â€“ https://opendatasus.saude.gov.br/

### Engenharia de Dados

- Designing Data-Intensive Applications â€“ Martin Kleppmann
- Spark: The Definitive Guide â€“ Bill Chambers

---

## ğŸ“– COMO CITAR

> IntroduÃ§Ã£o Ã  AnÃ¡lise de Dados em SaÃºde com PySpark. (2025). RepositÃ³rio GitHub. Brasil.

---

## ğŸ™ CRÃ‰DITOS

Base conceitual inspirada no curso **IntroduÃ§Ã£o Ã  AnÃ¡lise de Dados para Pesquisa no SUS** â€“ Fiocruz.

Dados: **Sistema de InformaÃ§Ãµes sobre Mortalidade (SIM) â€“ DATASUS**

---

**VersÃ£o:** 1.0  
**Ãšltima atualizaÃ§Ã£o:** Dezembro/2025

